{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Function to read the log file and filter lines containing \"(Test)\"\n",
    "def read_log_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Filter lines containing \"(Test)\"\n",
    "    test_logs = [line.strip() for line in lines if \"(Test)\" in line]\n",
    "    return test_logs\n",
    "\n",
    "# Function to capture 30 entries after every 120 entries\n",
    "def capture_entries(data_list, capture_size=30, skip_size=120, start_index=0):\n",
    "    captured_entries = []\n",
    "\n",
    "    # Loop through the data list with a step of 120\n",
    "    while start_index < len(data_list):\n",
    "        # Capture the next 30 entries\n",
    "        captured_entries.extend(data_list[start_index:start_index + capture_size])\n",
    "        # Move to the next block after skipping 120 entries\n",
    "        start_index += skip_size\n",
    "    \n",
    "    return captured_entries\n",
    "\n",
    "# Define a pattern to extract model metrics\n",
    "pattern = re.compile(\n",
    "    r\"INFO ([\\w-]+) \\(Test\\) - MAE: ([\\d.]+), MSE: ([\\d.]+), RMSE: ([\\d.]+), MAPE: ([\\d.]+), R2: ([\\d.-]+), Median AE: ([\\d.]+), Explained Variance: ([\\d.]+)\"\n",
    ")\n",
    "\n",
    "# Function to extract metrics from a list of log entries\n",
    "def extract_metrics_from_chunk(log_data_list):\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for log_entry in log_data_list:\n",
    "        match = pattern.search(log_entry)\n",
    "        if match:\n",
    "            model, mae, mse, rmse, mape, r2, median_ae, explained_variance = match.groups()\n",
    "            if model not in metrics_dict:\n",
    "                metrics_dict[model] = {\n",
    "                    \"MAE\": [],\n",
    "                    \"MSE\": [],\n",
    "                    \"RMSE\": [],\n",
    "                    \"MAPE\": [],\n",
    "                    \"R2\": [],\n",
    "                    \"Median AE\": [],\n",
    "                    \"Explained Variance\": []\n",
    "                }\n",
    "            \n",
    "            # Convert values to float and store them\n",
    "            metrics_dict[model][\"MAE\"].append(float(mae))\n",
    "            metrics_dict[model][\"MSE\"].append(float(mse))\n",
    "            metrics_dict[model][\"RMSE\"].append(float(rmse))\n",
    "            metrics_dict[model][\"MAPE\"].append(float(mape))\n",
    "            metrics_dict[model][\"R2\"].append(float(r2))\n",
    "            metrics_dict[model][\"Median AE\"].append(float(median_ae))\n",
    "            metrics_dict[model][\"Explained Variance\"].append(float(explained_variance))\n",
    "    \n",
    "    # Compute mean and std, keeping list features at the end\n",
    "    final_metrics_dict = {}\n",
    "    for model, metrics in metrics_dict.items():\n",
    "        mean_std_dict = {}\n",
    "        \n",
    "        # First, calculate mean and std\n",
    "        for metric, values in metrics.items():\n",
    "            mean_std_dict[f\"{metric}_mean\"] = mean(values)\n",
    "            mean_std_dict[f\"{metric}_std\"] = stdev(values) if len(values) > 1 else 0.0\n",
    "        \n",
    "        # Then, add the original list features at the end\n",
    "        for metric, values in metrics.items():\n",
    "            mean_std_dict[metric] = values\n",
    "        \n",
    "        # Store in the final dictionary\n",
    "        final_metrics_dict[model] = mean_std_dict\n",
    "\n",
    "    return final_metrics_dict\n",
    "\n",
    "\n",
    "# Function to process data in chunks of 30 entries and extract metrics for each chunk\n",
    "def process_in_chunks(data_list, chunk_size=30):\n",
    "    chunk_index = 1\n",
    "    df_list = []\n",
    "    for i in range(0, len(data_list), chunk_size):\n",
    "        chunk = data_list[i:i + chunk_size]\n",
    "        #print(f\"Chunk starting at index {i}:\")\n",
    "        \n",
    "        # Extract metrics for this chunk\n",
    "        chunk_metrics = extract_metrics_from_chunk(chunk)\n",
    "        \n",
    "        # Convert to DataFrame for better readability\n",
    "        chunk_metrics_df = pd.DataFrame(chunk_metrics).T\n",
    "        chunk_metrics_df.reset_index(inplace=True)\n",
    "        chunk_metrics_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "        chunk_metrics_df[\"index_count\"] = f'Experiment {chunk_index}'\n",
    "        df_list.append(chunk_metrics_df)\n",
    "        chunk_index += 1\n",
    "\n",
    "     # Concatenate all chunk DataFrames into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"C:/Users/rkbho/Downloads/metadata_regression_disjoint_data.log\"\n",
    "file_path = \"C:/Users/rkbho/Downloads/meta_data_regression_full.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\"MMCS0002\", \"MMCS0003\", \"MMCS0005\", \"MMCS0007\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "480\n",
      "(96, 26)\n",
      "480\n",
      "(96, 26)\n",
      "480\n",
      "(96, 26)\n",
      "480\n",
      "(96, 26)\n",
      "MMCS0002_forecasting\n",
      "MMCS0003_forecasting\n",
      "MMCS0005_forecasting\n",
      "MMCS0007_forecasting\n"
     ]
    }
   ],
   "source": [
    "features = [\"baseline_features\"]*24 + [\"++mean_and_std_features\"]*24 + [\"++lag_features\"]*24 + [\"++lag_difference_features\"]*24\n",
    "len(features)\n",
    "\n",
    "time_frames = [\"3hr\"]*6 + [\"6hr\"]*6 + [\"12hr\"]*6 + [\"24hr\"]*6\n",
    "final_time_frames = time_frames*4\n",
    "\n",
    "# Step 1: Read the log file and filter (Test) logs\n",
    "log_data_list = read_log_file(file_path)\n",
    "print(len(log_data_list))\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for index in range(0, len(id_list)): \n",
    "\n",
    "    # Step 2: Capture 30 entries after every 120 entries from the 2400 log entries\n",
    "    captured_entries = capture_entries(log_data_list, capture_size=30, skip_size=120, start_index=index*30)\n",
    "\n",
    "    print(len(captured_entries))\n",
    "    # Step 3: Extract metrics and calculate averages\n",
    "    mean_metrics_df = process_in_chunks(captured_entries)\n",
    "\n",
    "    mean_metrics_df[\"features_class\"] = features\n",
    "    mean_metrics_df[\"time_frame\"] = final_time_frames \n",
    "    mean_metrics_df[\"user_id\"] = id_list[index]\n",
    "\n",
    "    print(mean_metrics_df.shape)\n",
    "    str_key = f\"{id_list[index]}_forecasting\"\n",
    "    dataframes[str_key] = mean_metrics_df\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "with pd.ExcelWriter('forecasting_output.xlsx', engine='xlsxwriter') as writer:\n",
    "    for sheet_name, df in dataframes.items():\n",
    "        print(sheet_name)\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/rkbho/Downloads/meta_data_classification_full.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the log file and filter lines containing \"(Test)\"\n",
    "def read_log_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Filter lines containing \"(Test)\"\n",
    "    test_logs = [line.strip() for line in lines if \"(Test) - Accuracy\" in line]\n",
    "    return test_logs\n",
    "\n",
    "# Function to capture 30 entries after every 120 entries\n",
    "def capture_entries(data_list, capture_size=30, skip_size=120, start_index=0):\n",
    "    captured_entries = []\n",
    "\n",
    "    # Loop through the data list with a step of 120\n",
    "    while start_index < len(data_list):\n",
    "        # Capture the next 30 entries\n",
    "        captured_entries.extend(data_list[start_index:start_index + capture_size])\n",
    "        # Move to the next block after skipping 120 entries\n",
    "        start_index += skip_size\n",
    "    \n",
    "    return captured_entries\n",
    "\n",
    "# Define a pattern to extract model metrics for classification tasks\n",
    "pattern = re.compile(\n",
    "    r\"INFO ([\\w-]+) \\(Test\\) - Accuracy: ([\\d.]+), Precision: ([\\d.]+), Recall: ([\\d.]+), F1 Score: ([\\d.]+), AUC-ROC: ([\\d.]+)\"\n",
    ")\n",
    "\n",
    "def extract_metrics_from_chunk(log_data_list):\n",
    "    \"\"\"\n",
    "    Extracts metrics from a list of log entries and returns a dictionary with mean, std, and list of values.\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Extract metrics from log entries\n",
    "    for log_entry in log_data_list:\n",
    "        match = pattern.search(log_entry)\n",
    "        if match:\n",
    "            model, accuracy, precision, recall, f1_score, auc_roc = match.groups()\n",
    "            \n",
    "            # Initialize dictionary for the model if not already present\n",
    "            if model not in metrics_dict:\n",
    "                metrics_dict[model] = {\n",
    "                    \"Accuracy\": [], \n",
    "                    \"Precision\": [], \n",
    "                    \"Recall\": [], \n",
    "                    \"F1 Score\": [], \n",
    "                    \"AUC-ROC\": []\n",
    "                }\n",
    "            \n",
    "            # Convert values to float and store them in the dictionary\n",
    "            metrics_dict[model][\"Accuracy\"].append(float(accuracy))\n",
    "            metrics_dict[model][\"Precision\"].append(float(precision))\n",
    "            metrics_dict[model][\"Recall\"].append(float(recall))\n",
    "            metrics_dict[model][\"F1 Score\"].append(float(f1_score))\n",
    "            metrics_dict[model][\"AUC-ROC\"].append(float(auc_roc))\n",
    "    \n",
    "    # Compute mean and std, keeping list features at the end\n",
    "    final_metrics_dict = {}\n",
    "    for model, metrics in metrics_dict.items():\n",
    "        mean_std_dict = {}\n",
    "        \n",
    "        # First, calculate mean and std\n",
    "        for metric, values in metrics.items():\n",
    "            mean_std_dict[f\"{metric}_mean\"] = mean(values)\n",
    "            mean_std_dict[f\"{metric}_std\"] = stdev(values) if len(values) > 1 else 0.0\n",
    "        \n",
    "        # Then, add the original list features at the end\n",
    "        for metric, values in metrics.items():\n",
    "            mean_std_dict[metric] = values\n",
    "        \n",
    "        # Store in the final dictionary\n",
    "        final_metrics_dict[model] = mean_std_dict\n",
    "\n",
    "    return final_metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "480\n",
      "(96, 20)\n",
      "480\n",
      "(96, 20)\n",
      "480\n",
      "(96, 20)\n",
      "480\n",
      "(96, 20)\n",
      "MMCS0002_classification\n",
      "MMCS0003_classification\n",
      "MMCS0005_classification\n",
      "MMCS0007_classification\n"
     ]
    }
   ],
   "source": [
    "features = [\"baseline_features\"]*24 + [\"++mean_and_std_features\"]*24 + [\"++lag_features\"]*24 + [\"++lag_difference_features\"]*24\n",
    "len(features)\n",
    "\n",
    "time_frames = [\"3hr\"]*6 + [\"6hr\"]*6 + [\"12hr\"]*6 + [\"24hr\"]*6\n",
    "final_time_frames = time_frames*4\n",
    "\n",
    "# Step 1: Read the log file and filter (Test) logs\n",
    "log_data_list = read_log_file(file_path)\n",
    "print(len(log_data_list))\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for index in range(0, len(id_list)): \n",
    "\n",
    "    # Step 2: Capture 30 entries after every 120 entries from the 2400 log entries\n",
    "    captured_entries = capture_entries(log_data_list, capture_size=30, skip_size=120, start_index=index*30)\n",
    "\n",
    "    print(len(captured_entries))\n",
    "    # Step 3: Extract metrics and calculate averages\n",
    "    mean_metrics_df = process_in_chunks(captured_entries)\n",
    "\n",
    "    mean_metrics_df[\"features_class\"] = features\n",
    "    mean_metrics_df[\"time_frame\"] = final_time_frames \n",
    "    mean_metrics_df[\"user_id\"] = id_list[index]\n",
    "\n",
    "    print(mean_metrics_df.shape)\n",
    "    str_key = f\"{id_list[index]}_classification\"\n",
    "    dataframes[str_key] = mean_metrics_df\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "with pd.ExcelWriter('classification_output.xlsx', engine='xlsxwriter') as writer:\n",
    "    for sheet_name, df in dataframes.items():\n",
    "        print(sheet_name)\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
